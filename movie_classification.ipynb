{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3a729c",
   "metadata": {},
   "source": [
    "#### PySpark Project Documentation: Movie Revenue Prediction Pipeline\n",
    "\n",
    "##### This step-by-step guide walks beginners through building a machine learning pipeline in PySpark using a movie dataset. \n",
    "\n",
    "##### The objective is to predict the revenue category of a movie based on features like ratings, views, and awards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92b3f8",
   "metadata": {},
   "source": [
    " Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491bbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyFirstSparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27b22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96550fd6",
   "metadata": {},
   "source": [
    "Load Dataset\n",
    "\n",
    "Read a CSV dataset into a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cb7e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+------+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+\n",
      "|               title|    country|              genres|            language|writer_count|title_adaption|censor_rating|release_date|runtime|dvd_release_date|users_votes|comments| likes|overall_views|dislikes|ratings_imdb|ratings_tomatoes|ratings_metacritic|special_award|awards_win|awards_nomination|revenue_category|\n",
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+------+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+\n",
      "|Pooh's Heffalump ...|        USA|Animation, Comedy...|             English|         3.0|          True|            G|   11-Feb-05| 68 min|       24-May-05|      5,654|    NULL|  NULL|         NULL|    NULL|      6.4/10|             80%|            64/100|            0|         0|                1|            High|\n",
      "|Yeh Jawaani Hai D...|      India|Comedy, Drama, Mu...|Hindi, English, F...|         2.0|          True|    Not Rated|   31-May-13|160 min|       15-Jul-13|     33,860|     9.0| 124.0|     127528.0|    12.0|      7.1/10|             67%|                 0|            0|        22|               92|             Low|\n",
      "|Tae Guk Gi: The B...|South Korea|  Action, Drama, War|              Korean|         4.0|         False|            R|   24-Sep-04|140 min|       15-Feb-05|     35,697|   268.0| 614.0|     351123.0|    18.0|      8.1/10|             80%|            64/100|            0|        12|                7|             Low|\n",
      "|   Book of Eli, The |        USA|Action, Adventure...|             English|         1.0|         False|            R|   15-Jan-10|118 min|       15-Jun-10|   2,71,524|    NULL|  NULL|         NULL|    NULL|      6.9/10|             47%|            53/100|            0|         3|               16|            High|\n",
      "|       Blind Dating |        USA|     Comedy, Romance|             English|         1.0|         False|        PG-13|   26-Apr-07| 95 min|       05-Feb-08|     11,251|   116.0| 693.0|     970306.0|    36.0|      6.1/10|             25%|                 0|            0|         0|                0|             Low|\n",
      "|Sweetest Thing, The |        USA|     Comedy, Romance|             English|         1.0|         False|            R|   12-Apr-02| 88 min|       20-Aug-02|     52,005|    41.0| 353.0|     547201.0|    33.0|      5.1/10|             26%|            32/100|            0|         1|                4|            High|\n",
      "|            Jet Lag | France, UK|     Comedy, Romance|              French|         2.0|         False|            R|   30-Oct-02| 91 min|       20-Jan-04|      5,498|    NULL|  NULL|         NULL|    NULL|      6.2/10|             57%|            53/100|            0|         0|                1|             Low|\n",
      "|Journey to the Ce...|        USA|Action, Adventure...|English, Icelandi...|         4.0|          True|           PG|   11-Jul-08| 93 min|       28-Oct-08|   1,04,379|     1.0|  56.0|      54156.0|     9.0|      5.8/10|             61%|            57/100|            0|         3|                3|            High|\n",
      "|              Radio |        USA|Biography, Drama,...|             English|         1.0|         False|           PG|   24-Oct-03|109 min|       27-Jan-04|     35,956|   159.0|1224.0|     355936.0|    57.0|      6.9/10|             36%|            38/100|            0|         3|                5|            High|\n",
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+------+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true').csv('train.csv')\n",
    "df.show(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e867af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- writer_count: string (nullable = true)\n",
      " |-- title_adaption: string (nullable = true)\n",
      " |-- censor_rating: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- dvd_release_date: string (nullable = true)\n",
      " |-- users_votes: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- overall_views: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- ratings_imdb: string (nullable = true)\n",
      " |-- ratings_tomatoes: string (nullable = true)\n",
      " |-- ratings_metacritic: string (nullable = true)\n",
      " |-- special_award: string (nullable = true)\n",
      " |-- awards_win: string (nullable = true)\n",
      " |-- awards_nomination: string (nullable = true)\n",
      " |-- revenue_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cf942",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "\n",
    "a. Clean Rating Columns\n",
    "\n",
    "Extract numeric values from mixed rating formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a8d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i.They have mixed types like 7.1/10, 64/100, 80%. Let’s clean them:\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "df = df.withColumn(\"ratings_imdb\", regexp_extract(\"ratings_imdb\", r\"([\\d\\.]+)\", 1).cast(\"float\"))\n",
    "df = df.withColumn(\"ratings_metacritic\", regexp_extract(\"ratings_metacritic\", r\"([\\d\\.]+)\", 1).cast(\"float\"))\n",
    "df = df.withColumn(\"ratings_tomatoes\", regexp_extract(\"ratings_tomatoes\", r\"([\\d\\.]+)\", 1).cast(\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d215598",
   "metadata": {},
   "source": [
    "b. Clean Runtime and Count Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f2d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Convert runtime from \"min\" to int\n",
    "df = df.withColumn(\"runtime\", regexp_extract(\"runtime\", r\"(\\d+)\", 1).cast(\"int\"))\n",
    "\n",
    "# iii. Handle votes and views with commas\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df = df.withColumn(\"users_votes\", regexp_replace(\"users_votes\", \",\", \"\").cast(\"int\"))\n",
    "df = df.withColumn(\"overall_views\", regexp_replace(\"overall_views\", \",\", \"\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae59aef",
   "metadata": {},
   "source": [
    "c. Parse and Extract Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "033e56c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+-----------------------+------------+\n",
      "|               title|    country|              genres|            language|writer_count|title_adaption|censor_rating|release_date|runtime|dvd_release_date|users_votes|comments|likes|overall_views|dislikes|ratings_imdb|ratings_tomatoes|ratings_metacritic|special_award|awards_win|awards_nomination|revenue_category|dvd_release_date_parsed|release_year|\n",
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+-----------------------+------------+\n",
      "|Pooh's Heffalump ...|        USA|Animation, Comedy...|             English|         3.0|          True|            G|   11-Feb-05|     68|       24-May-05|       5654|    NULL| NULL|         NULL|    NULL|         6.4|            80.0|              64.0|            0|         0|                1|            High|             2005-05-24|        2005|\n",
      "|Yeh Jawaani Hai D...|      India|Comedy, Drama, Mu...|Hindi, English, F...|         2.0|          True|    Not Rated|   31-May-13|    160|       15-Jul-13|      33860|     9.0|124.0|       127528|    12.0|         7.1|            67.0|               0.0|            0|        22|               92|             Low|             2013-07-15|        2013|\n",
      "|Tae Guk Gi: The B...|South Korea|  Action, Drama, War|              Korean|         4.0|         False|            R|   24-Sep-04|    140|       15-Feb-05|      35697|   268.0|614.0|       351123|    18.0|         8.1|            80.0|              64.0|            0|        12|                7|             Low|             2005-02-15|        2005|\n",
      "|   Book of Eli, The |        USA|Action, Adventure...|             English|         1.0|         False|            R|   15-Jan-10|    118|       15-Jun-10|     271524|    NULL| NULL|         NULL|    NULL|         6.9|            47.0|              53.0|            0|         3|               16|            High|             2010-06-15|        2010|\n",
      "|       Blind Dating |        USA|     Comedy, Romance|             English|         1.0|         False|        PG-13|   26-Apr-07|     95|       05-Feb-08|      11251|   116.0|693.0|       970306|    36.0|         6.1|            25.0|               0.0|            0|         0|                0|             Low|             2008-02-05|        2008|\n",
      "|Sweetest Thing, The |        USA|     Comedy, Romance|             English|         1.0|         False|            R|   12-Apr-02|     88|       20-Aug-02|      52005|    41.0|353.0|       547201|    33.0|         5.1|            26.0|              32.0|            0|         1|                4|            High|             2002-08-20|        2002|\n",
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+-----------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year\n",
    "\n",
    "# Step 1: Convert the string to a proper date format\n",
    "df = df.withColumn(\"dvd_release_date_parsed\", to_date(\"dvd_release_date\", \"dd-MMM-yy\"))\n",
    "\n",
    "# Step 2: Extract the year from the parsed date\n",
    "df = df.withColumn(\"release_year\", year(\"dvd_release_date_parsed\"))\n",
    "\n",
    "# Show result\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f4e25",
   "metadata": {},
   "source": [
    "d. Show summary statistics of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "307112cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------------+--------------------+-----------------+--------------+-------------+------------+------------------+----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+----------------+-----------------+\n",
      "|summary|               title|             country|           genres|            language|     writer_count|title_adaption|censor_rating|release_date|           runtime|dvd_release_date|       users_votes|          comments|             likes|    overall_views|          dislikes|      ratings_imdb|  ratings_tomatoes|ratings_metacritic|      special_award|       awards_win| awards_nomination|revenue_category|     release_year|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+-----------------+--------------+-------------+------------+------------------+----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+----------------+-----------------+\n",
      "|  count|                2398|                2400|             2400|                2400|             2306|          2400|         2362|        2396|              2400|            2331|              2400|              1974|              1956|             2083|              1956|              2400|              2400|              2400|               2400|             2400|              2400|            2400|             2331|\n",
      "|   mean|               756.8|                NULL|             NULL|                NULL|2.596270598438855|          NULL|         NULL|        NULL|          105.8825|            NULL| 82958.68208333333| 546.4381965552178|3113.8941717791413|1345471.603456553|232.66564417177915| 6.493916667550803| 56.85458333333333|53.775416666666665|0.18541666666666667|6.582916666666667|12.698333333333334|            NULL|2009.042042042042|\n",
      "| stddev|   921.3265979010918|                NULL|             NULL|                NULL|2.011585524938396|          NULL|         NULL|        NULL|20.624280047307817|            NULL|148688.12313797796|2103.8331644312398|10213.600410572713|4030918.298482433|  946.288417162496|0.9995895799081576|28.148361259471564|21.002569857876612|0.38871620089842635|16.08985683430561|  24.9259645169486|            NULL|6.818677954498133|\n",
      "|    min|                '71 |           Argentina|           Action|Aboriginal, Engli...|              1.0|         False|            G|   01-Apr-05|                38|       01-Apr-03|                55|               0.0|               0.0|               21|               0.0|               1.7|               0.0|               0.0|                  0|                0|                 0|            High|             2000|\n",
      "|    max|xXx: State of the...|United Arab Emira...|Thriller, Western|Zulu, Xhosa, Afri...|              9.0|          True|      Unrated|   31-Oct-15|               566|       31-Oct-06|           2070977|             997.0|            9985.0|        107150221|             996.0|               9.0|             100.0|             100.0|                  1|               99|                99|             Low|             2099|\n",
      "+-------+--------------------+--------------------+-----------------+--------------------+-----------------+--------------+-------------+------------+------------------+----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507540c6",
   "metadata": {},
   "source": [
    "Type Casting and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5a52cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('writer_count', df['writer_count'].cast(IntegerType()))\n",
    "df = df.withColumn('likes', df['likes'].cast(IntegerType()))\n",
    "df = df.withColumn('dislikes', df['dislikes'].cast(IntegerType()))\n",
    "df = df.withColumn('special_award', df['special_award'].cast(IntegerType()))\n",
    "df = df.withColumn('awards_win', df['awards_win'].cast(IntegerType()))\n",
    "df = df.withColumn('awards_nomination', df['awards_nomination'].cast(IntegerType()))\n",
    "\n",
    "df = df.withColumn(\n",
    "    'ratings',\n",
    "    (df['ratings_imdb'] + df['ratings_metacritic']/10 + df['ratings_tomatoes']/10) / 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736ad8e",
   "metadata": {},
   "source": [
    " Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a09a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric nulls\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "for col_name in numeric_cols:\n",
    "    df = df.fillna({col_name: 0})\n",
    "\n",
    "# Fill categorical nulls with \"Unknown\"\n",
    "df = df.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548d055",
   "metadata": {},
   "source": [
    "Feature Engineering and ML Pipeline\n",
    "\n",
    "a. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cdbe204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: Index categorical columns\n",
    "indexer = StringIndexer(inputCol='revenue_category', outputCol='label')\n",
    "df = indexer.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "331a1a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+-----------------------+------------+------------------+-----+\n",
      "|               title|    country|              genres|            language|writer_count|title_adaption|censor_rating|release_date|runtime|dvd_release_date|users_votes|comments|likes|overall_views|dislikes|ratings_imdb|ratings_tomatoes|ratings_metacritic|special_award|awards_win|awards_nomination|revenue_category|dvd_release_date_parsed|release_year|           ratings|label|\n",
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+-----------------------+------------+------------------+-----+\n",
      "|Pooh's Heffalump ...|        USA|Animation, Comedy...|             English|           3|          True|            G|   11-Feb-05|     68|       24-May-05|       5654| Unknown|    0|            0|       0|         6.4|            80.0|              64.0|            0|         0|                1|            High|             2005-05-24|        2005| 6.933333365122476|  0.0|\n",
      "|Yeh Jawaani Hai D...|      India|Comedy, Drama, Mu...|Hindi, English, F...|           2|          True|    Not Rated|   31-May-13|    160|       15-Jul-13|      33860|     9.0|  124|       127528|      12|         7.1|            67.0|               0.0|            0|        22|               92|             Low|             2013-07-15|        2013| 4.599999968210856|  1.0|\n",
      "|Tae Guk Gi: The B...|South Korea|  Action, Drama, War|              Korean|           4|         False|            R|   24-Sep-04|    140|       15-Feb-05|      35697|   268.0|  614|       351123|      18|         8.1|            80.0|              64.0|            0|        12|                7|             Low|             2005-02-15|        2005| 7.500000127156575|  1.0|\n",
      "|   Book of Eli, The |        USA|Action, Adventure...|             English|           1|         False|            R|   15-Jan-10|    118|       15-Jun-10|     271524| Unknown|    0|            0|       0|         6.9|            47.0|              53.0|            0|         3|               16|            High|             2010-06-15|        2010|5.6333333651224775|  0.0|\n",
      "+--------------------+-----------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+----------------+-----------------------+------------+------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32d4f0",
   "metadata": {},
   "source": [
    "After applying String Indexer to the target column, \n",
    "Low has been replaced with 1 \n",
    "while High = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cbbeaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Step 2: One-Hot Encode the indexed features (not the label)\\nencoders = [\\n    OneHotEncoder(inputCol='title_idx', outputCol='title_org'),\\n    OneHotEncoder(inputCol='censor_idx', outputCol='censor_org')\\n]\\n\\ndf_encoded = encoders\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Step 2: One-Hot Encode the indexed features (not the label)\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol='title_idx', outputCol='title_org'),\n",
    "    OneHotEncoder(inputCol='censor_idx', outputCol='censor_org')\n",
    "]\n",
    "\n",
    "df_encoded = encoders\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38102ba3",
   "metadata": {},
   "source": [
    "b. Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f800c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature columns\n",
    "feature_cols = [\n",
    "    'writer_count', 'runtime', 'users_votes',\n",
    "    'likes', 'overall_views', 'dislikes', 'special_award',\n",
    "    'awards_win', 'awards_nomination', 'ratings', 'release_year'\n",
    "]\n",
    "\n",
    "# Step 4: Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcc678",
   "metadata": {},
   "source": [
    " Model Training and Evaluation\n",
    "\n",
    "a. Train/Test Split and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define model\n",
    "model = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\"\"\"\n",
    "# Step 6: Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, model])\"\"\"\n",
    "\n",
    "# Step 7: Train-test split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ca06cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(train_data)\n",
    "\n",
    "prediction = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05f23a",
   "metadata": {},
   "source": [
    "b. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cf3fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9254191817572105\n"
     ]
    }
   ],
   "source": [
    "# Find AUC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:\"areaUnderROC\"})\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42c06c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.846\n",
      "Precision: 0.847\n",
      "Recall: 0.846\n",
      "F1 Score: 0.846\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(prediction, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(prediction, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(prediction, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ae53f",
   "metadata": {},
   "source": [
    "Load and Predict on New Data\n",
    "\n",
    "Import new data and carry out same data cleaning and feature engineering done on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9427f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-------+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+\n",
      "|               title|     country|              genres|            language|writer_count|title_adaption|censor_rating|release_date|runtime|dvd_release_date|users_votes|comments|  likes|overall_views|dislikes|ratings_imdb|ratings_tomatoes|ratings_metacritic|special_award|awards_win|awards_nomination|\n",
      "+--------------------+------------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-------+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+\n",
      "|            Delhi-6 |       India|       Comedy, Drama|      Hindi, English|         3.0|          True|    Not Rated|   20-Feb-09|141 min|       16-Jun-09|      6,337|     7.0|   43.0|     165008.0|    13.0|      6.0/10|             40%|            45/100|            0|         2|                7|\n",
      "| Before I Disappear |     USA, UK|               Drama|             English|         1.0|         False|      Unrated|   28-Nov-14| 93 min|       19-May-15|      8,370|    NULL|   NULL|         NULL|    NULL|      7.2/10|             38%|            47/100|            0|        16|                8|\n",
      "|       Good Year, A |     USA, UK|Comedy, Drama, Ro...|English, French, ...|         2.0|          True|        PG-13|   10-Nov-06|117 min|       27-Feb-07|     82,062|    14.0|  276.0|     187788.0|    17.0|      7.0/10|             25%|            47/100|            0|         1|                2|\n",
      "|              Brüno |     USA, UK|              Comedy|     English, German|         9.0|          True|            R|   10-Jul-09| 81 min|       17-Nov-09|   1,33,925|    81.0|  301.0|     200324.0|    43.0|      5.8/10|             67%|            54/100|            0|         2|                5|\n",
      "|How to Lose a Guy...|USA, Germany|     Comedy, Romance|             English|         5.0|          True|        PG-13|   07-Feb-03|116 min|       01-Jul-03|   1,90,396|    74.0|  480.0|     740345.0|    23.0|      6.4/10|             42%|            45/100|            0|         1|                8|\n",
      "|           Restless |         USA|      Drama, Romance|             English|         1.0|         False|        PG-13|   06-Apr-11| 91 min|       24-Jan-12|     16,698|    NULL|   NULL|         NULL|    NULL|      6.8/10|             37%|            47/100|            0|         0|                1|\n",
      "|  Young & Beautiful |      France|      Drama, Romance|      French, German|         1.0|         False|    Not Rated|   25-Apr-14| 95 min|       26-Aug-14|     28,841|    63.0|  518.0|     871034.0|   162.0|      6.8/10|             73%|            63/100|            0|         3|                7|\n",
      "|         Iron Man 3 |         USA|Action, Adventure...|             English|         8.0|          True|        PG-13|   03-May-13|130 min|       24-Sep-13|   6,93,723|  2664.0|19551.0|    3984203.0|   799.0|      7.2/10|             79%|            62/100|            1|        17|               61|\n",
      "|           Invictus |         USA|Biography, Drama,...|English, Afrikaan...|         2.0|          True|        PG-13|   11-Dec-09|134 min|       18-May-10|   1,41,004|     0.0|    7.0|      13204.0|     2.0|      7.3/10|             76%|            74/100|            1|        10|               33|\n",
      "+--------------------+------------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-------+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.option('header','true').csv('test.csv')\n",
    "data.show(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8cc8de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+-----------------------+------------+\n",
      "|               title|     country|              genres|            language|writer_count|title_adaption|censor_rating|release_date|runtime|dvd_release_date|users_votes|comments|likes|overall_views|dislikes|ratings_imdb|ratings_tomatoes|ratings_metacritic|special_award|awards_win|awards_nomination|dvd_release_date_parsed|release_year|\n",
      "+--------------------+------------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+-----------------------+------------+\n",
      "|            Delhi-6 |       India|       Comedy, Drama|      Hindi, English|         3.0|          True|    Not Rated|   20-Feb-09|    141|       16-Jun-09|       6337|     7.0| 43.0|       165008|    13.0|         6.0|            40.0|              45.0|            0|         2|                7|             2009-06-16|        2009|\n",
      "| Before I Disappear |     USA, UK|               Drama|             English|         1.0|         False|      Unrated|   28-Nov-14|     93|       19-May-15|       8370|    NULL| NULL|         NULL|    NULL|         7.2|            38.0|              47.0|            0|        16|                8|             2015-05-19|        2015|\n",
      "|       Good Year, A |     USA, UK|Comedy, Drama, Ro...|English, French, ...|         2.0|          True|        PG-13|   10-Nov-06|    117|       27-Feb-07|      82062|    14.0|276.0|       187788|    17.0|         7.0|            25.0|              47.0|            0|         1|                2|             2007-02-27|        2007|\n",
      "|              Brüno |     USA, UK|              Comedy|     English, German|         9.0|          True|            R|   10-Jul-09|     81|       17-Nov-09|     133925|    81.0|301.0|       200324|    43.0|         5.8|            67.0|              54.0|            0|         2|                5|             2009-11-17|        2009|\n",
      "|How to Lose a Guy...|USA, Germany|     Comedy, Romance|             English|         5.0|          True|        PG-13|   07-Feb-03|    116|       01-Jul-03|     190396|    74.0|480.0|       740345|    23.0|         6.4|            42.0|              45.0|            0|         1|                8|             2003-07-01|        2003|\n",
      "|           Restless |         USA|      Drama, Romance|             English|         1.0|         False|        PG-13|   06-Apr-11|     91|       24-Jan-12|      16698|    NULL| NULL|         NULL|    NULL|         6.8|            37.0|              47.0|            0|         0|                1|             2012-01-24|        2012|\n",
      "+--------------------+------------+--------------------+--------------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+-----------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace, to_date, year\n",
    "\n",
    "# i. They have mixed types like 7.1/10, 64/100, 80%. Let’s clean them\n",
    "data = data.withColumn(\"ratings_imdb\", regexp_extract(\"ratings_imdb\", r\"([\\d\\.]+)\", 1).cast(\"float\"))\n",
    "data = data.withColumn(\"ratings_metacritic\", regexp_extract(\"ratings_metacritic\", r\"([\\d\\.]+)\", 1).cast(\"float\"))\n",
    "data = data.withColumn(\"ratings_tomatoes\", regexp_extract(\"ratings_tomatoes\", r\"([\\d\\.]+)\", 1).cast(\"float\"))\n",
    "\n",
    "# ii. Convert runtime from \"min\" to int\n",
    "data = data.withColumn(\"runtime\", regexp_extract(\"runtime\", r\"(\\d+)\", 1).cast(\"int\"))\n",
    "\n",
    "# iii. Handle votes and views with commas\n",
    "data = data.withColumn(\"users_votes\", regexp_replace(\"users_votes\", \",\", \"\").cast(\"int\"))\n",
    "data = data.withColumn(\"overall_views\", regexp_replace(\"overall_views\", \",\", \"\").cast(\"int\"))\n",
    "\n",
    "# Step 1: Convert the string to a proper date format\n",
    "data = data.withColumn(\"dvd_release_date_parsed\", to_date(\"dvd_release_date\", \"dd-MMM-yy\"))\n",
    "\n",
    "# Step 2: Extract the year from the parsed date\n",
    "data = data.withColumn(\"release_year\", year(\"dvd_release_date_parsed\"))\n",
    "\n",
    "# Show result\n",
    "data.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da3603a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, NumericType\n",
    "\n",
    "# Cast columns to IntegerType\n",
    "data = data.withColumn('writer_count', data['writer_count'].cast(IntegerType()))\n",
    "data = data.withColumn('likes', data['likes'].cast(IntegerType()))\n",
    "data = data.withColumn('dislikes', data['dislikes'].cast(IntegerType()))\n",
    "data = data.withColumn('special_award', data['special_award'].cast(IntegerType()))\n",
    "data = data.withColumn('awards_win', data['awards_win'].cast(IntegerType()))\n",
    "data = data.withColumn('awards_nomination', data['awards_nomination'].cast(IntegerType()))\n",
    "\n",
    "# Create the 'ratings' column\n",
    "data = data.withColumn(\n",
    "    'ratings',\n",
    "    (data['ratings_imdb'] + data['ratings_metacritic']/10 + data['ratings_tomatoes']/10) / 3\n",
    ")\n",
    "\n",
    "# Fill numeric nulls\n",
    "numeric_cols = [f.name for f in data.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "for col_name in numeric_cols:\n",
    "    data = data.fillna({col_name: 0})\n",
    "\n",
    "# Fill categorical nulls with \"Unknown\"\n",
    "data = data.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecb9e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'writer_count', 'runtime', 'users_votes',\n",
    "    'likes', 'overall_views', 'dislikes', 'special_award',\n",
    "    'awards_win', 'awards_nomination', 'ratings', 'release_year'\n",
    "]\n",
    "\n",
    "# Step 4: Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69532807",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24e474be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+--------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+-----------------------+------------+-----------------+--------------------+--------------------+--------------------+----------+\n",
      "|              title|country|       genres|      language|writer_count|title_adaption|censor_rating|release_date|runtime|dvd_release_date|users_votes|comments|likes|overall_views|dislikes|ratings_imdb|ratings_tomatoes|ratings_metacritic|special_award|awards_win|awards_nomination|dvd_release_date_parsed|release_year|          ratings|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+-------+-------------+--------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+-----------------------+------------+-----------------+--------------------+--------------------+--------------------+----------+\n",
      "|           Delhi-6 |  India|Comedy, Drama|Hindi, English|           3|          True|    Not Rated|   20-Feb-09|    141|       16-Jun-09|       6337|     7.0|   43|       165008|      13|         6.0|            40.0|              45.0|            0|         2|                7|             2009-06-16|        2009|4.833333333333333|[3.0,141.0,6337.0...|[5.13747854267790...|[0.25687392713389...|       1.0|\n",
      "|Before I Disappear |USA, UK|        Drama|       English|           1|         False|      Unrated|   28-Nov-14|     93|       19-May-15|       8370| Unknown|    0|            0|       0|         7.2|            38.0|              47.0|            0|        16|                8|             2015-05-19|        2015|5.233333269755046|[1.0,93.0,8370.0,...|[3.41061672243418...|[0.17053083612170...|       1.0|\n",
      "+-------------------+-------+-------------+--------------+------------+--------------+-------------+------------+-------+----------------+-----------+--------+-----+-------------+--------+------------+----------------+------------------+-------------+----------+-----------------+-----------------------+------------+-----------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e97482f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|  289|\n",
      "|       1.0|  311|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_data.groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29aff89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "predict_data = predict_data.withColumn(\n",
    "    'revenue_category',\n",
    "    when(predict_data['prediction'] == 0, 'High').otherwise('Low')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2397892",
   "metadata": {},
   "source": [
    "Export to a csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e2412",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"test_data_to_pandas = predict_data.select('title', 'revenue_category').toPandas()\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mtest_data_to_pandas.to_csv('final_submission.csv', index=False)\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[33;03mOR \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mpredict_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrevenue_category\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfinal_sub\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Projects\\Spark\\first_spark\\spark_venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   1847\u001b[39m     compression=compression,\n\u001b[32m   1848\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1862\u001b[39m     lineSep=lineSep,\n\u001b[32m   1863\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Projects\\Spark\\first_spark\\spark_venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Projects\\Spark\\first_spark\\spark_venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\Projects\\Spark\\first_spark\\spark_venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1024.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "test_data_to_pandas = predict_data.select('title', 'revenue_category').toPandas()\n",
    "test_data_to_pandas.to_csv('final_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f6466b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m.stop()\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098f45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
